# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/04_user_guide/04_data_catalog.html

raw_car_location:
  type: spark.SparkDataSet
  file_format: 'csv'
  filepath: "${data_path}/01_raw/sf_location.csv"
  load_args:
    header: True
    inferSchema: False
    schema: >
      position_id integer,
      driver_id string,
      latitude double,
      longitude double,
      time double,
      source string
    multiline: True

raw_car_crash:
  type: spark.SparkDataSet
  file_format: 'csv'
  filepath: "${data_path}/01_raw/sf_crash.csv"
  load_args:
    header: True
    inferSchema: False
    schema: >
      driver_id string,
      time double,
      crash integer,
      source string
    multiline: True

prm_car_crash:
  type: spark.SparkDataSet
  filepath: "${data_path}/03_primary/car_crash"
  file_format: 'parquet'
  save_args:
    mode: "overwrite"

prm_car_location:
  type: spark.SparkDataSet
  filepath: "${data_path}/03_primary/car_location"
  file_format: 'parquet'
  save_args:
    mode: "overwrite"

prm_spine:
  type: spark.SparkDataSet
  filepath: "${data_path}/03_primary/spine"
  file_format: 'parquet'
  save_args:
    mode: "overwrite"

fea_total_miles:
  type: spark.SparkDataSet
  filepath: "${data_path}/04_features/total_miles"
  file_format: 'parquet'
  save_args:
    mode: "overwrite"

mod_master:
  type: spark.SparkDataSet
  filepath: "${data_path}/05_model_input/master"
  file_format: 'parquet'
  save_args:
    mode: "overwrite"
